{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a97cd228",
   "metadata": {},
   "source": [
    "# Assignment 03\n",
    "\n",
    "### Deadline: Next tutorial\n",
    "### Deliverables: Quiz on this assignment in the next tutorial\n",
    "\n",
    "In this assignment, you can use the same virtual environment used for the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c500784",
   "metadata": {},
   "source": [
    "## 1. Symbolic Regression\n",
    "\n",
    "### 1.1 ECSEL as symbolic regressor\n",
    "First we look at ECSEL as symbolic regressor, thus where the ground truth equation is known, and we try to recover it exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde452c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and settings\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06d46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def create_term_sparse(const: float, var_powers: dict[int, float]):\n",
    "    \"\"\"Sparse term: const * Π x_var^power, where var indexes start at 1.\"\"\"\n",
    "    return {\"const\": float(const), \"var_powers\": dict(var_powers)}\n",
    "\n",
    "def sparse_to_dense_terms(sparse_terms):\n",
    "    \"\"\"\n",
    "    Converts sparse terms to dense terms with exponent vectors of length m.\n",
    "    Returns: (dense_terms, m)\n",
    "    \"\"\"\n",
    "    if len(sparse_terms) == 0:\n",
    "        raise ValueError(\"No terms provided.\")\n",
    "\n",
    "    max_var = 0\n",
    "    for term in sparse_terms:\n",
    "        if term[\"var_powers\"]:\n",
    "            max_var = max(max_var, max(term[\"var_powers\"].keys()))\n",
    "    if max_var == 0:\n",
    "        max_var = 1  # handle constant-only terms\n",
    "\n",
    "    dense_terms = []\n",
    "    for term in sparse_terms:\n",
    "        exps = np.zeros(max_var, dtype=float)\n",
    "        for var_idx, power in term[\"var_powers\"].items():\n",
    "            exps[var_idx - 1] = float(power)\n",
    "        dense_terms.append({\"const\": float(term[\"const\"]), \"exponents\": exps})\n",
    "\n",
    "    return dense_terms, max_var\n",
    "\n",
    "def evaluate_terms(X, dense_terms):\n",
    "    \"\"\"\n",
    "    Evaluates each term on each sample.\n",
    "    X: (n, m)\n",
    "    dense_terms: list of {\"const\": c, \"exponents\": (m,)}\n",
    "    returns term_values: (n, K)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    n, m = X.shape\n",
    "    K = len(dense_terms)\n",
    "    term_vals = np.zeros((n, K), dtype=float)\n",
    "\n",
    "    # keep strictly positive for non-integer exponents + log\n",
    "    X_safe = np.clip(X, 1e-12, None)\n",
    "\n",
    "    for k, term in enumerate(dense_terms):\n",
    "        c = term[\"const\"]\n",
    "        e = term[\"exponents\"]\n",
    "        # Π x_j^e_j\n",
    "        term_vals[:, k] = c * np.prod(X_safe ** e, axis=1)\n",
    "\n",
    "    return term_vals\n",
    "\n",
    "def generate_synthetic_data(n_samples, sparse_terms, x_bounds=(0.1, 5.0), noise_std=0.0, seed=42):\n",
    "    \"\"\"\n",
    "    Generates y = sum_k c_k * Π x_j^{e_jk} + noise\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dense_terms, m = sparse_to_dense_terms(sparse_terms)\n",
    "\n",
    "    X = rng.uniform(x_bounds[0], x_bounds[1], size=(n_samples, m))\n",
    "    y = evaluate_terms(X, dense_terms).sum(axis=1)\n",
    "\n",
    "    if noise_std > 0:\n",
    "        y = y + rng.normal(0.0, noise_std, size=n_samples)\n",
    "\n",
    "    return X, y, dense_terms\n",
    "\n",
    "def params_to_terms(params, K, m):\n",
    "    per = m + 1\n",
    "    dense_terms = []\n",
    "    for k in range(K):\n",
    "        s = k * per\n",
    "        c = params[s]\n",
    "        e = params[s+1:s+1+m]\n",
    "        dense_terms.append({\"const\": float(c), \"exponents\": np.array(e, dtype=float)})\n",
    "    return dense_terms\n",
    "\n",
    "def print_sparse_terms(sparse_terms, title=\"TRUE FUNCTION\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for i, t in enumerate(sparse_terms, 1):\n",
    "        c = t[\"const\"]\n",
    "        vp = t[\"var_powers\"]\n",
    "        if not vp:\n",
    "            term = \"1\"\n",
    "        else:\n",
    "            term = \" * \".join([f\"x{v}^{p:g}\" for v, p in sorted(vp.items())])\n",
    "        sign = \"+\" if c >= 0 else \"\"\n",
    "        print(f\"  Term {i}: {sign}{c:g} * {term}\")\n",
    "\n",
    "def print_dense_terms(dense_terms, title=\"FITTED TERMS\", max_terms=None):\n",
    "    print(f\"\\n{title}:\")\n",
    "    shown = dense_terms if max_terms is None else dense_terms[:max_terms]\n",
    "    for i, t in enumerate(shown, 1):\n",
    "        c = t[\"const\"]\n",
    "        e = t[\"exponents\"]\n",
    "        parts = [f\"x{j+1}^{e[j]:.3f}\" for j in range(len(e)) if abs(e[j]) > 1e-6]\n",
    "        term = \" * \".join(parts) if parts else \"1\"\n",
    "        sign = \"+\" if c >= 0 else \"\"\n",
    "        print(f\"  Term {i}: {sign}{c:.3f} * {term}\")\n",
    "\n",
    "def reconstruction_metrics(X, y, dense_terms):\n",
    "    y_hat = evaluate_terms(X, dense_terms).sum(axis=1)\n",
    "    mse = np.mean((y - y_hat) ** 2)\n",
    "    rel = np.mean(np.abs(y - y_hat) / (np.abs(y) + 1e-12))\n",
    "    return mse, rel, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a541b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECSEL\n",
    "\n",
    "def unpack_params(params, K, m):\n",
    "    \"\"\"params -> list of (c_k, e_k) where e_k is length m\"\"\"\n",
    "    params = np.asarray(params, dtype=float)\n",
    "    per = m + 1\n",
    "    terms = []\n",
    "    for k in range(K):\n",
    "        s = k * per\n",
    "        c = params[s]\n",
    "        e = params[s+1:s+1+m]\n",
    "        terms.append((c, e))\n",
    "    return terms\n",
    "\n",
    "def objective_mse(params, X, y, K, m):\n",
    "    try:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        n = y.shape[0]\n",
    "        X_safe = np.clip(X, 1e-12, None)\n",
    "\n",
    "        y_pred = np.zeros(n, dtype=float)\n",
    "        for c, e in unpack_params(params, K, m):\n",
    "            y_pred += c * np.prod(X_safe ** e, axis=1)\n",
    "\n",
    "        mse = np.mean((y - y_pred) ** 2)\n",
    "        if not np.isfinite(mse):\n",
    "            return 1e10\n",
    "        return mse\n",
    "    except Exception:\n",
    "        return 1e10\n",
    "\n",
    "def grad_mse(params, X, y, K, m):\n",
    "    \"\"\"\n",
    "    Analytical gradient of MSE wrt constants + exponents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        n = y.shape[0]\n",
    "        X_safe = np.clip(X, 1e-12, None)\n",
    "        logX = np.log(X_safe)\n",
    "\n",
    "        per = m + 1\n",
    "        grad = np.zeros_like(params, dtype=float)\n",
    "\n",
    "        # forward\n",
    "        term_vals = []\n",
    "        y_pred = np.zeros(n, dtype=float)\n",
    "        terms = unpack_params(params, K, m)\n",
    "        for (c, e) in terms:\n",
    "            t = c * np.prod(X_safe ** e, axis=1)\n",
    "            term_vals.append(t)\n",
    "            y_pred += t\n",
    "\n",
    "        r = (y_pred - y)  # residual\n",
    "        scale = 2.0 / n\n",
    "\n",
    "        # backward\n",
    "        for k, (c, e) in enumerate(terms):\n",
    "            s = k * per\n",
    "            base = np.prod(X_safe ** e, axis=1)  # without c\n",
    "\n",
    "            # d/dc_k\n",
    "            grad[s] = scale * np.sum(r * base)\n",
    "\n",
    "            # d/de_kj\n",
    "            # term = c * base; d(term)/de_j = term * log(x_j)\n",
    "            term = c * base\n",
    "            for j in range(m):\n",
    "                grad[s + 1 + j] = scale * np.sum(r * term * logX[:, j])\n",
    "\n",
    "        # guard\n",
    "        grad = np.where(np.isfinite(grad), grad, 0.0)\n",
    "        return grad\n",
    "    except Exception:\n",
    "        return np.ones_like(params, dtype=float) * 1e10\n",
    "    \n",
    "def fit_lbfgsb(X, y, K, n_restarts=10, bounds_const=(-10, 10), bounds_exp=(-5, 5), seed=42, use_grad=True):\n",
    "    \"\"\"\n",
    "    Fits K-term signomial with L-BFGS-B and random restarts.\n",
    "    params layout: [c1, e11..e1m, c2, e21..e2m, ...]\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n, m = X.shape\n",
    "    per = m + 1\n",
    "    n_params = K * per\n",
    "\n",
    "    bounds = []\n",
    "    for _ in range(K):\n",
    "        bounds.append(bounds_const)\n",
    "        bounds.extend([bounds_exp] * m)\n",
    "\n",
    "    best = None\n",
    "    best_val = np.inf\n",
    "\n",
    "    print(f\"L-BFGS-B: {n_restarts} restarts | K={K}, m={m}\")\n",
    "    for r in range(n_restarts):\n",
    "        x0 = np.zeros(n_params, dtype=float)\n",
    "\n",
    "        # heuristic init for restart 0\n",
    "        if r == 0:\n",
    "            for k in range(K):\n",
    "                s = k * per\n",
    "                x0[s] = rng.uniform(-1, 1)              # small const\n",
    "                x0[s+1:s+1+m] = rng.uniform(0.5, 2.0, size=m)  # positive exponents\n",
    "        else:\n",
    "            for i, (lb, ub) in enumerate(bounds):\n",
    "                x0[i] = rng.uniform(lb, ub)\n",
    "\n",
    "        try:\n",
    "            res = minimize(\n",
    "                fun=lambda p: objective_mse(p, X, y, K, m),\n",
    "                x0=x0,\n",
    "                method=\"L-BFGS-B\",\n",
    "                jac=(lambda p: grad_mse(p, X, y, K, m)) if use_grad else None,\n",
    "                bounds=bounds,\n",
    "                options={\"maxiter\": 1000, \"ftol\": 1e-12}\n",
    "            )\n",
    "            if res.success and res.fun < best_val:\n",
    "                best_val = res.fun\n",
    "                best = res.x.copy()\n",
    "                print(f\"  restart {r+1}/{n_restarts}: improved -> MSE {best_val:.8e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  restart {r+1}/{n_restarts}: failed ({e})\")\n",
    "\n",
    "    return best, best_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c65ea",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "We apply symbolic regression on signomials of the AI Feynman database of equations (see related literature Udrescu & Tegmark, 2019). We choose equations I.12.1, I.12.2 and I.13.12 and generate 200 samples between 0.1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1d9d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set\n",
    "\n",
    "test_equations = {\n",
    "    \"I.12.1\": [\n",
    "        create_term_sparse(1.0, {1: 1.0, 2: 1.0, 3: 1.0}),\n",
    "    ],\n",
    "    \"I.12.2\": [\n",
    "        create_term_sparse(1.0, {1: 1.0, 2: 1.0, 3: -2.0, 4: -1.0}),\n",
    "    ],\n",
    "    \"I.13.12\": [\n",
    "        create_term_sparse(1.0, {1: 1.0, 2: 1.0, 3: 1.0, 5: -1.0}),\n",
    "        create_term_sparse(-1.0, {1: 1.0, 2: 1.0, 3: 1.0, 4: -1.0}),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53976297",
   "metadata": {},
   "source": [
    "The following cell will run the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cff0856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FITTING EQUATION: I.12.1\n",
      "\n",
      "TRUE FUNCTION:\n",
      "  Term 1: +1 * x1^1 * x2^1 * x3^1\n",
      "\n",
      "Data: n=200, m=3 | y range [0.099, 93.828]\n",
      "L-BFGS-B: 15 restarts | K=1, m=3\n",
      "  restart 1/15: improved -> MSE 1.10239351e-04\n",
      "  restart 4/15: improved -> MSE 1.10239351e-04\n",
      "\n",
      "FITTED TERMS:\n",
      "  Term 1: +1.000 * x1^1.000 * x2^1.000 * x3^1.000\n",
      "\n",
      "MSE: 1.10239351e-04\n",
      "Mean relative error: 0.003775 (0.377%)\n",
      "Fit time: 0.10s | Total time: 0.10s\n",
      "\n",
      "================================================================================\n",
      "FITTING EQUATION: I.12.2\n",
      "\n",
      "TRUE FUNCTION:\n",
      "  Term 1: +1 * x1^1 * x2^1 * x3^-2 * x4^-1\n",
      "\n",
      "Data: n=200, m=4 | y range [-0.009, 422.965]\n",
      "L-BFGS-B: 15 restarts | K=1, m=4\n",
      "  restart 1/15: improved -> MSE 1.93462294e+03\n",
      "  restart 2/15: improved -> MSE 9.95035846e-05\n",
      "  restart 4/15: improved -> MSE 9.95035846e-05\n",
      "\n",
      "FITTED TERMS:\n",
      "  Term 1: +1.000 * x1^1.000 * x2^1.000 * x3^-2.000 * x4^-1.000\n",
      "\n",
      "MSE: 9.95035846e-05\n",
      "Mean relative error: 0.128648 (12.865%)\n",
      "Fit time: 0.09s | Total time: 0.09s\n",
      "\n",
      "================================================================================\n",
      "FITTING EQUATION: I.13.12\n",
      "\n",
      "TRUE FUNCTION:\n",
      "  Term 1: +1 * x1^1 * x2^1 * x3^1 * x5^-1\n",
      "  Term 2: -1 * x1^1 * x2^1 * x3^1 * x4^-1\n",
      "\n",
      "Data: n=200, m=5 | y range [-143.339, 228.122]\n",
      "L-BFGS-B: 15 restarts | K=2, m=5\n",
      "  restart 1/15: improved -> MSE 8.39946252e-05\n",
      "  restart 2/15: improved -> MSE 8.39946233e-05\n",
      "\n",
      "FITTED TERMS:\n",
      "  Term 1: -1.000 * x1^1.000 * x2^1.000 * x3^1.000 * x4^-1.000 * x5^0.000\n",
      "  Term 2: +1.000 * x1^1.000 * x2^1.000 * x3^1.000 * x4^0.000 * x5^-1.000\n",
      "\n",
      "MSE: 8.39946233e-05\n",
      "Mean relative error: 0.142436 (14.244%)\n",
      "Fit time: 1.18s | Total time: 1.18s\n"
     ]
    }
   ],
   "source": [
    "def run_experiments(\n",
    "    equations: dict,\n",
    "    n_samples=200,\n",
    "    noise_std=0.01,\n",
    "    n_restarts=12,\n",
    "    x_bounds=(0.1, 5.0),\n",
    "    seed=42\n",
    "):\n",
    "    for name, sparse_terms in equations.items():\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"FITTING EQUATION: {name}\")\n",
    "        start = time.time()\n",
    "\n",
    "        K = len(sparse_terms)\n",
    "        X, y, _ = generate_synthetic_data(\n",
    "            n_samples=n_samples,\n",
    "            sparse_terms=sparse_terms,\n",
    "            x_bounds=x_bounds,\n",
    "            noise_std=noise_std,\n",
    "            seed=seed\n",
    "        )\n",
    "        n, m = X.shape\n",
    "\n",
    "        print_sparse_terms(sparse_terms)\n",
    "        print(f\"\\nData: n={n}, m={m} | y range [{y.min():.3f}, {y.max():.3f}]\")\n",
    "\n",
    "        fit_start = time.time()\n",
    "        best_params, best_obj = fit_lbfgsb(\n",
    "            X, y, K,\n",
    "            n_restarts=n_restarts,\n",
    "            seed=seed,\n",
    "            use_grad=True\n",
    "        )\n",
    "        fit_time = time.time() - fit_start\n",
    "\n",
    "        if best_params is None:\n",
    "            print(\"Optimization failed.\")\n",
    "            continue\n",
    "\n",
    "        fitted_terms = params_to_terms(best_params, K, m)\n",
    "        print_dense_terms(fitted_terms, title=\"FITTED TERMS\")\n",
    "\n",
    "        mse, rel, _ = reconstruction_metrics(X, y, fitted_terms)\n",
    "        print(f\"\\nMSE: {mse:.8e}\")\n",
    "        print(f\"Mean relative error: {rel:.6f} ({rel*100:.3f}%)\")\n",
    "        print(f\"Fit time: {fit_time:.2f}s | Total time: {time.time()-start:.2f}s\")\n",
    "        \n",
    "        \n",
    "np.random.seed(42)\n",
    "run_experiments(test_equations, n_samples=200, noise_std=0.01, n_restarts=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4d6232",
   "metadata": {},
   "source": [
    "#### Q1. What is the difference between equations I.12.1, I.12.2 and I.13.12?\n",
    "\n",
    "#### Q2. Find an equation in the database (see AI Feynman paper) that is NOT recovered using ECSEL. Can you think of a reason why?\n",
    "\n",
    "#### Q3. Scan the paper (ECSEL, Lumadjeng et al. (2026)). What would be the elasticity of equation I.12.1?\n",
    "\n",
    "#### Q4. Can we read the elasticity as easy off of equations I.13.12? Explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0bca3",
   "metadata": {},
   "source": [
    "### 1.2 ECSEL as classifier\n",
    "\n",
    "We use ECSEL on the Online Shopping Intention dataset. This is a dataset of ~12K samples/users that browser an online shopping website. The task is to predict whether a user session will end in a purchase or not. The equations found by ECSEL is given by\n",
    "\n",
    "<div>\n",
    "<img src=\"../img/osi_ecsel_signomial.png\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "where the sigmoid threshold is equal to p=0.559\n",
    "\n",
    "Q5. Explain what this threshold means\n",
    "\n",
    "Q6. Looking at the equation, what can you say about how predictions are made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0eb432",
   "metadata": {},
   "source": [
    "Q7. Give a global explanation using an elasticity plot. Explain what we see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd26c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Q7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16993092",
   "metadata": {},
   "source": [
    "Q8. Give a local explanation via a waterfall plot. Choose an instance yourself. Explain what we see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f52c763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4794138",
   "metadata": {},
   "source": [
    "## 2. Counterfactual Explanations Using Optimization with Constraint Learning (CE-OCL)\n",
    "\n",
    "Read the paper (main text is 6 pages). You will be quizzed on comprehension, methodology and interpretability of results. You can find the paper in the directory, or go to Canvas > Modules > Related Literature of this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbee6a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_week4 (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
